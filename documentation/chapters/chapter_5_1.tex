%
\section{Evaluation}
\label{sec_eval}

\subsection{Benchmarks}
To evaluate our tool, we created a benchmark set containing several images where we manually marked the insects.
The result of one benchmark run is expressed as \textit{Recall}, \textit{Precision}, and \textit{F-Measure} \cite{f_measure}.
Although F-Measure is a very common way to measure the quality of results, we consider changing  to another method that puts a higher weight on the quantity of results. 
Usually, it would be easier to refine existing results than to find additional insects. 
Also, when looking for false positives, it is possible that true positives are found because the bounding box did not fit correctly.
However, it is more useful to see an image of multiple insects or a cropped insect instead of no result at all.

The test set is crucial for the accuracy of the benchmark. 
It is always good to have a large test set to improve the accuracy of the benchmark.
Therefore continuous extending of the test set is needed. 

\subsection{Results}

The results show nothing!!