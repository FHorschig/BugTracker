%
\section{Evaluation}
\label{sec_eval}

\subsection{Benchmarks}
To evaluate our tool, we created a benchmark set containing several images where we manually marked the insects.
The result of one benchmark run is expressed as \textit{Recall}, \textit{Precision}, and \textit{F-Measure} \cite{f_measure}.
Although F-Measure is a very common way to measure the quality of results, we consider changing  to another method that puts a higher weight on the quantity of results. 
Usually, it would be easier to refine existing results than to find additional insects. 
Also, when looking for false positives, it is possible that true positives are found because the bounding box did not fit correctly.
However, it is more useful to see an image of multiple insects or a cropped insect instead of no result at all.

The test set is crucial for the accuracy of the benchmark. 
It is always good to have a large test set to improve the accuracy of the benchmark.
Therefore continuous extending of the test set is needed. 

\subsection{Results}

Running the benchmark on our test set yields the following results:
%
\begin{description}
	\item[Recall] 0.63
	\item[Precision] 0.67
	\item[F-Measure] 0.65
\end{description}
%
The used method consisted of the automated template extraction by contour detection, followed by a template matching with the extracted template.
